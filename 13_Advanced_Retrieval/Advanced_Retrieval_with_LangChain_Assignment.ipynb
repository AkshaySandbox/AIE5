{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- 🤝 Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- 🤝 Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank)).\n",
        "\n",
        "> You do not need to run the following cells if you are running this notebook locally. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgFAXWVW3wm",
        "outputId": "636db35c-f05a-4038-ec7a-02360bef2dae"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain langchain-openai langchain-cohere rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKqYM4Eoxcov"
      },
      "source": [
        "We're also going to be leveraging [Qdrant's](https://qdrant.tech/documentation/frameworks/langchain/) (pronounced \"Quadrant\") VectorDB in \"memory\" mode (so we can leverage it locally in our colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6xav5CxYnML"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0pDRFEWSXvh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using some reviews from the 4 movies in the John Wick franchise today to explore the different retrieval strategies.\n",
        "\n",
        "These were obtained from IMDB, and are available in the [AIM Data Repository](https://github.com/AI-Maker-Space/DataRepository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKHcZmKzDwT"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We can simply `wget` these from GitHub.\n",
        "\n",
        "You could use any review data you wanted in this step - just be careful to make sure your metadata is aligned with your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbbSIGtzX3dS",
        "outputId": "0ce6514e-2479-4001-af24-824f987ce599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-04 14:13:27--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19628 (19K) [text/plain]\n",
            "Saving to: ‘john_wick_1.csv’\n",
            "\n",
            "john_wick_1.csv     100%[===================>]  19.17K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-03-04 14:13:28 (34.2 MB/s) - ‘john_wick_1.csv’ saved [19628/19628]\n",
            "\n",
            "--2025-03-04 14:13:28--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14747 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_2.csv’\n",
            "\n",
            "john_wick_2.csv     100%[===================>]  14.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-04 14:13:28 (60.1 MB/s) - ‘john_wick_2.csv’ saved [14747/14747]\n",
            "\n",
            "--2025-03-04 14:13:29--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13888 (14K) [text/plain]\n",
            "Saving to: ‘john_wick_3.csv’\n",
            "\n",
            "john_wick_3.csv     100%[===================>]  13.56K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-04 14:13:29 (43.4 MB/s) - ‘john_wick_3.csv’ saved [13888/13888]\n",
            "\n",
            "--2025-03-04 14:13:29--  https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15109 (15K) [text/plain]\n",
            "Saving to: ‘john_wick_4.csv’\n",
            "\n",
            "john_wick_4.csv     100%[===================>]  14.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-04 14:13:30 (35.1 MB/s) - ‘john_wick_4.csv’ saved [15109/15109]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw1.csv -O john_wick_1.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw2.csv -O john_wick_2.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw3.csv -O john_wick_3.csv\n",
        "!wget https://raw.githubusercontent.com/AI-Maker-Space/DataRepository/main/jw4.csv -O john_wick_4.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today.\n",
        "\n",
        "- Self-Query: Wants as much metadata as we can provide\n",
        "- Time-weighted: Wants temporal data\n",
        "\n",
        "> NOTE: While we're creating a temporal relationship based on when these movies came out for illustrative purposes, it needs to be clear that the \"time-weighting\" in the Time-weighted Retriever is based on when the document was *accessed* last - not when it was created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(1, 5):\n",
        "  loader = CSVLoader(\n",
        "      file_path=f\"john_wick_{i}.csv\",\n",
        "      metadata_columns=[\"Review_Date\", \"Review_Title\", \"Review_Url\", \"Author\", \"Rating\"]\n",
        "  )\n",
        "\n",
        "  movie_docs = loader.load()\n",
        "  for doc in movie_docs:\n",
        "\n",
        "    # Add the \"Movie Title\" (John Wick 1, 2, ...)\n",
        "    doc.metadata[\"Movie_Title\"] = f\"John Wick {i}\"\n",
        "\n",
        "    # convert \"Rating\" to an `int`, if no rating is provided - assume 0 rating\n",
        "    doc.metadata[\"Rating\"] = int(doc.metadata[\"Rating\"]) if doc.metadata[\"Rating\"] else 0\n",
        "\n",
        "    # newer movies have a more recent \"last_accessed_at\"\n",
        "    doc.metadata[\"last_accessed_at\"] = datetime.now() - timedelta(days=4-i)\n",
        "\n",
        "  documents.extend(movie_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2025, 3, 1, 14, 13, 30, 551666)}, page_content=\": 0\\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.\")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"JohnWick\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWick\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-3.5-turbo` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the reviews provided.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3\". Here is the URL to that review: \\'/review/rw4854296/?ref_=tt_urv\\''"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to track down the gangsters that killed his dog and took everything from him. This leads to a series of intense action, shootouts, and fights as he seeks revenge against those responsible.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People's opinions on John Wick vary. Some loved it for its action sequences and stylish stunts, while others found it boring and plotless. It seems to be a movie that polarizes audiences.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I'm sorry, there are no reviews with a rating of 10 in the context provided.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, the main character, played by Keanu Reeves, is a retired hitman seeking revenge for the killing of his puppy, which was a final gift from his recently deceased wife. The movie follows his journey as he takes on the criminal underworld.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse - but the `I don't know` isn't great!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick based on the positive reviews provided in the context.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. The review with a rating of 10 is for the movie \"John Wick 3\" by the author \\'ymyuseda\\'. The URL to that review is \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, after resolving his issues with the Russian mafia, John Wick is asked to help by Santino D'Antonio, but he refuses. As a result, Santino blows up his house. John Wick then meets Winston, the owner of the Continental hotel, who tells him he must honor the marker and kills Santino's sister in Rome. This leads to a contract being placed on John Wick, attracting professional killers. Wick promises to kill Santino, who is no longer protected by his marker.\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, people generally liked John Wick.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I\\'m sorry, there are no reviews with a rating of 10 for the movie \"John Wick 4.\"'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick\" movies, the story follows a retired assassin named John Wick who is pulled back into the world of violence and revenge. In the first movie, John seeks vengeance against the gangsters who killed his dog and stole his car. In the sequels, he gets caught up in the criminal underworld and faces new challenges that force him to resort to his lethal skills. The films are known for their intense action sequences and stylish presentation.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = documents\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/t6/07vbm7893sncjgwzstdk9cyw0000gn/T/ipykernel_41568/3574430551.py:8: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
            "  parent_document_vectorstore = Qdrant(\n"
          ]
        }
      ],
      "source": [
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = Qdrant(\n",
        "    collection_name=\"full_documents\", embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"People's opinions on John Wick seem to be divided based on the reviews provided. Some individuals seem to dislike the movie, while others greatly enjoy it.\""
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10. Here is the URL to that review: /review/rw4854296/?ref_=tt_urv'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In John Wick, retired assassin John Wick comes out of retirement to seek vengeance when someone kills his dog and steals his car. He is then forced to pay off an old debt by helping Ian McShane take over the Assassin's Guild, leading to a lot of carnage and numerous killings of assassins in Italy, Canada, and Manhattan.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that a majority of people liked John Wick. Reviews highlighted the excellent action sequences, Keanu Reeves' performance, and the overall enjoyment of the film. The positive feedback indicates that people generally enjoyed John Wick.\""
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is one review with a rating of 10 for the movie \"John Wick 3.\" Here is the URL to that review: /review/rw4854296/?ref_=tt_urv.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In John Wick, an ex-hitman comes out of retirement to seek vengeance on the gangsters that killed his dog and stole everything from him. He unleashes a maelstrom of destruction against his enemies, leading to a relentless vendetta against those who wronged him. The movie is filled with loud action, suspense, and intense fights, making it a gripping and violent story.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!\n",
        "\n",
        "> NOTE: You do not need to run this cell if you're running this locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dHeB-yGXneL",
        "outputId": "efc59105-518a-4134-9228-d98b8a97e08e"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"JohnWickSemantic\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the reviews provided, it seems that people generally liked John Wick. The majority of reviews are positive, praising the action sequences, Keanu Reeves' performance, and the overall entertainment value of the movie.\""
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Did people generally like John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there is a review with a rating of 10 for the movie \"John Wick 3.\" The URL to that review is: \\'/review/rw4854296/?ref_=tt_urv\\'.'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Do any reviews have a rating of 10? If so - can I have the URLs to those reviews?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'In \"John Wick,\" the main character, John Wick, seeks revenge on the people who took something he loved from him. It\\'s a story of action, stylish stunts, kinetic chaos, and a relatable hero seeking vengeance.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What happened in John Wick?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### 🏗️ Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tgDICngKXLGK"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE \n",
        "\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79f96a311a944a4a9111417f4c706d26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/44 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f12715de30714f55820cfd309481f7ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Node e821b0ac-d650-450f-9934-156ef9a7f605 does not have a summary. Skipping filtering.\n",
            "Node 348f8f8d-7e12-4ce8-97d4-e83da55d422f does not have a summary. Skipping filtering.\n",
            "Node 2addc01a-db88-4c51-9a0a-a4f4c7a3c4d0 does not have a summary. Skipping filtering.\n",
            "Node 7bc0bde8-a348-4731-8354-7edcac2e7121 does not have a summary. Skipping filtering.\n",
            "Node ea8dce1d-6c7a-47a2-8c04-0d562fa71da8 does not have a summary. Skipping filtering.\n",
            "Node 430c1fbb-72be-4df6-9790-884e0f23c5f2 does not have a summary. Skipping filtering.\n",
            "Node e41aefe7-de8e-41a6-938a-968a2f989c61 does not have a summary. Skipping filtering.\n",
            "Node 89bbd6f4-1814-40c5-8989-63cf48f2d557 does not have a summary. Skipping filtering.\n",
            "Node da83aeed-98d3-4564-90db-fc9fe8042050 does not have a summary. Skipping filtering.\n",
            "Node e2f9e3e0-2387-4503-87f6-d80fe405ef32 does not have a summary. Skipping filtering.\n",
            "Node 32475680-ef04-45ac-9b19-f029de6f0091 does not have a summary. Skipping filtering.\n",
            "Node f8ca6d5a-2400-417d-9481-346e083f80a5 does not have a summary. Skipping filtering.\n",
            "Node 8b88ee8a-c071-463f-9d7b-f97f352b81df does not have a summary. Skipping filtering.\n",
            "Node 95b3a4c0-6de1-4376-ac48-2e09a2447942 does not have a summary. Skipping filtering.\n",
            "Node 1fa6b2e0-a866-4491-8c54-53b6f0d8b5a4 does not have a summary. Skipping filtering.\n",
            "Node 437e670b-4e15-4a00-9bb0-ce383a18458f does not have a summary. Skipping filtering.\n",
            "Node 07aab26d-6f84-491a-ba99-0533dcfccd99 does not have a summary. Skipping filtering.\n",
            "Node 8ab1d312-7275-47ff-9b80-0f398f45e658 does not have a summary. Skipping filtering.\n",
            "Node b58b37ca-11a4-492c-8dee-fd0dfcb0bd62 does not have a summary. Skipping filtering.\n",
            "Node d16526bd-3d8b-4061-a932-b873313afa3c does not have a summary. Skipping filtering.\n",
            "Node 9cae5dac-8cb7-4d90-b5eb-2d7a57febf5e does not have a summary. Skipping filtering.\n",
            "Node 7b4ce035-7d75-425f-a24c-4d45f4b9f9ca does not have a summary. Skipping filtering.\n",
            "Node 6d587f98-da0c-4047-9824-e6d19bf84226 does not have a summary. Skipping filtering.\n",
            "Node c3cf2c6e-645e-4ed8-bbb7-0af62616faa1 does not have a summary. Skipping filtering.\n",
            "Node ceb63d1a-291e-4f2c-84f8-6e37b9d6517b does not have a summary. Skipping filtering.\n",
            "Node 5f0f497c-f87e-48d3-87b0-89e5b3bb86f7 does not have a summary. Skipping filtering.\n",
            "Node 4c361aeb-5fdd-41e5-9b9c-12536069bee7 does not have a summary. Skipping filtering.\n",
            "Node c32b7eaf-af79-4e2c-ae12-03194b64dec4 does not have a summary. Skipping filtering.\n",
            "Node fe7b341a-a3bf-4bee-bb0f-fe5e08e6b320 does not have a summary. Skipping filtering.\n",
            "Node a20f1743-7657-4ba0-b0d3-5f3c0ac25aa5 does not have a summary. Skipping filtering.\n",
            "Node f3e4878a-fbe5-4da0-832f-a18d00814243 does not have a summary. Skipping filtering.\n",
            "Node d04f2ef0-2466-4184-a364-b68ea9d96a49 does not have a summary. Skipping filtering.\n",
            "Node 5c73a489-7ef3-48aa-8005-76115dfd4065 does not have a summary. Skipping filtering.\n",
            "Node d5dab4df-1563-4234-8c6e-8056f5639f2c does not have a summary. Skipping filtering.\n",
            "Node ea3fc66b-49ce-41b7-a482-45e1f5804b02 does not have a summary. Skipping filtering.\n",
            "Node c96ba48e-c978-4dfd-9503-b0498142e74d does not have a summary. Skipping filtering.\n",
            "Node f59dbc9a-25a3-411c-bc9a-b4a1b099af87 does not have a summary. Skipping filtering.\n",
            "Node 9f88154f-e77f-41ba-b107-b7351d890f67 does not have a summary. Skipping filtering.\n",
            "Node 9c7fafed-9a67-477f-9686-a43bfa25d5f3 does not have a summary. Skipping filtering.\n",
            "Node 5d81afc9-ac18-42d2-b35f-eb4978c7ec05 does not have a summary. Skipping filtering.\n",
            "Node ba75d4d3-c28a-44d2-90c4-c47c4e491ab7 does not have a summary. Skipping filtering.\n",
            "Node d3d2c8cd-e497-4238-9cda-db003f58d1c1 does not have a summary. Skipping filtering.\n",
            "Node fdea62fa-dea1-46e1-a8f3-12eb3cee1f9c does not have a summary. Skipping filtering.\n",
            "Node de90aeaa-f0e6-41b2-9379-44bd15b90ed7 does not have a summary. Skipping filtering.\n",
            "Node a6cbfeb7-bb8b-4e43-bb59-c1d845bc470b does not have a summary. Skipping filtering.\n",
            "Node a6491c41-63ac-49ce-9f00-73234a1d339e does not have a summary. Skipping filtering.\n",
            "Node 600dcdab-65da-4aaf-a20c-9ee013d8f71b does not have a summary. Skipping filtering.\n",
            "Node 50fd3cc4-afe9-4796-a94b-2bbb3480e17b does not have a summary. Skipping filtering.\n",
            "Node c1cfca1e-1360-4d94-9bee-4c063a888f02 does not have a summary. Skipping filtering.\n",
            "Node c0d85c6f-effa-4af1-a7a9-b93b13c1c308 does not have a summary. Skipping filtering.\n",
            "Node e9c4a5c8-5238-410e-85f3-49aef39ece72 does not have a summary. Skipping filtering.\n",
            "Node 3770d545-fd9e-4e92-92c7-356f5012bf17 does not have a summary. Skipping filtering.\n",
            "Node 506627a7-f3c9-4c36-9b06-0b822632df3a does not have a summary. Skipping filtering.\n",
            "Node 897d977c-7b01-4bd6-aeec-c583b4f5795a does not have a summary. Skipping filtering.\n",
            "Node 08dcdbcd-ad82-4546-8982-929eedcdcf52 does not have a summary. Skipping filtering.\n",
            "Node 3e4b7805-d91e-4c09-9044-88e942eff239 does not have a summary. Skipping filtering.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52077b3d8d9742ffbc72d6347762d02a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/244 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b85306c67cd045cea69d18255c98b9c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "307b21e210474f41a1cd04add764b419",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb4c8a32b572463ea1a4028dd79d4255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4b6b9e42b044f16a0793948615d51bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...   \n",
              "1  The John Wick film series is apparently loved ...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie, retired assassin John Wick suffe...   \n",
              "4  In John Wick, gangsters are central to the plo...   \n",
              "5  The creators of John Wick 3 struggled with fin...   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...   \n",
              "7  The key differences in the reception of John W...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  Keaunu's performance in John Wick is special b...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  multi_hop_specific_query_synthesizer  \n",
              "6  multi_hop_specific_query_synthesizer  \n",
              "7  multi_hop_specific_query_synthesizer  \n",
              "8  multi_hop_specific_query_synthesizer  \n",
              "9  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(documents, testset_size=10)\n",
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"RAGAS_APP_TOKEN\"] = getpass(\"Please enter your Ragas API key!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Retrieval \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "939e470db593407488ed745ce12b3ca1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9500, 'context_entity_recall': 0.4954, 'noise_sensitivity_relevant': 0.4534, 'context_precision': 0.8340}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = naive_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "from ragas import EvaluationDataset\n",
        "\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 14\\nReview: Keanu Reeve is John Wick. He's ...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, a ...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.891723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The general reception of the John Wick film se...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.792857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.796190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 18\\nReview: When the story begins, John (Ke...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick,\" John experiences a s...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.778333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the plot of John Wick, gangsters play a sig...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.545455</td>\n",
              "      <td>0.924036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>Based on the reviews provided, both 'John Wick...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.926667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>The reception of John Wick 2 seems to vary amo...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.702778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[: 4\\nReview: I went to the cinema with great ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.924036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 20\\nReview: John Wick is something special....</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keaunu's performance in John Wick is special c...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.936735</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 14\\nReview: Keanu Reeve is John Wick. He's ...   \n",
              "1  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 18\\nReview: When the story begins, John (Ke...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 22\\nReview: Lets contemplate about componen...   \n",
              "6  [: 0\\nReview: The best way I can describe John...   \n",
              "7  [: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...   \n",
              "8  [: 4\\nReview: I went to the cinema with great ...   \n",
              "9  [: 20\\nReview: John Wick is something special....   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves plays the character John Wick, a ...   \n",
              "1  The general reception of the John Wick film se...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick,\" John experiences a s...   \n",
              "4  In the plot of John Wick, gangsters play a sig...   \n",
              "5                                      I don't know.   \n",
              "6  Based on the reviews provided, both 'John Wick...   \n",
              "7  The reception of John Wick 2 seems to vary amo...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  Keaunu's performance in John Wick is special c...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...             1.0   \n",
              "1  The John Wick film series is apparently loved ...             1.0   \n",
              "2  Keanu Reeves' performance in John Wick stands ...             1.0   \n",
              "3  In the movie, retired assassin John Wick suffe...             1.0   \n",
              "4  In John Wick, gangsters are central to the plo...             1.0   \n",
              "5  The creators of John Wick 3 struggled with fin...             0.5   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...             1.0   \n",
              "7  The key differences in the reception of John W...             1.0   \n",
              "8  The criticisms of the action sequences in 'Par...             1.0   \n",
              "9  Keaunu's performance in John Wick is special b...             1.0   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               1.000000                    0.666667           0.891723  \n",
              "1               0.000000                    0.000000           0.792857  \n",
              "2               1.000000                    1.000000           0.796190  \n",
              "3               0.500000                    0.600000           0.778333  \n",
              "4               0.125000                    0.545455           0.924036  \n",
              "5               0.428571                    0.500000           0.666667  \n",
              "6               0.400000                    0.666667           0.926667  \n",
              "7               0.333333                    0.444444           0.702778  \n",
              "8               0.500000                    0.000000           0.924036  \n",
              "9               0.666667                    0.111111           0.936735  "
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: Naive Retrieval\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|   Index |   Context Recall |   Context Precision |   Noise Sensitivity Relevant |\n",
            "+=========+==================+=====================+==============================+\n",
            "|       1 |              1   |            0.891723 |                     0.666667 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       2 |              1   |            0.792857 |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       3 |              1   |            0.79619  |                     1        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       4 |              1   |            0.778333 |                     0.6      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       5 |              1   |            0.924036 |                     0.545455 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       6 |              0.5 |            0.666667 |                     0.5      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       7 |              1   |            0.926667 |                     0.666667 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       8 |              1   |            0.702778 |                     0.444444 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       9 |              1   |            0.924036 |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|      10 |              1   |            0.936735 |                     0.111111 |\n",
            "+---------+------------------+---------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "from tabulate import tabulate\n",
        "# Creating a table with headers\n",
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Retrieval method: Naive Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BM25 Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4517c2a133a45368b2d82de0be1d3fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.4667, 'context_entity_recall': 0.4210, 'noise_sensitivity_relevant': 0.3256, 'context_precision': 0.5000}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = bm25_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, ContextEntityRecall, NoiseSensitivity, ContextPrecision\n",
        "from ragas import evaluate, RunConfig\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 11\\nReview: Who needs a 2hr and 40 min acti...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves is the actor who plays the titula...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The general reception of the John Wick film se...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 5\\nReview: What is all the raving about wit...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>I'm sorry, I don't have specific information o...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 11\\nReview: Who needs a 2hr and 40 min acti...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>Gangsters play a significant role in the plot ...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I don't have specific information on what elem...</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>In terms of storytelling and action elements, ...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[: 16\\nReview: John Wick Chapter 2 pits Keanu ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the reviews provided, the key differe...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[: 18\\nReview: Ever since the original John Wi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>I don't know the specific criticisms of the ac...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>In John Wick, Keanu's performance is special b...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 11\\nReview: Who needs a 2hr and 40 min acti...   \n",
              "1  [: 20\\nReview: In a world where movie sequels ...   \n",
              "2  [: 22\\nReview: Lets contemplate about componen...   \n",
              "3  [: 5\\nReview: What is all the raving about wit...   \n",
              "4  [: 11\\nReview: Who needs a 2hr and 40 min acti...   \n",
              "5  [: 22\\nReview: Lets contemplate about componen...   \n",
              "6  [: 20\\nReview: In a world where movie sequels ...   \n",
              "7  [: 16\\nReview: John Wick Chapter 2 pits Keanu ...   \n",
              "8  [: 18\\nReview: Ever since the original John Wi...   \n",
              "9  [: 22\\nReview: Lets contemplate about componen...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves is the actor who plays the titula...   \n",
              "1  The general reception of the John Wick film se...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  I'm sorry, I don't have specific information o...   \n",
              "4  Gangsters play a significant role in the plot ...   \n",
              "5  I don't have specific information on what elem...   \n",
              "6  In terms of storytelling and action elements, ...   \n",
              "7  Based on the reviews provided, the key differe...   \n",
              "8  I don't know the specific criticisms of the ac...   \n",
              "9  In John Wick, Keanu's performance is special b...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...        1.000000   \n",
              "1  The John Wick film series is apparently loved ...        0.000000   \n",
              "2  Keanu Reeves' performance in John Wick stands ...        0.333333   \n",
              "3  In the movie, retired assassin John Wick suffe...        0.000000   \n",
              "4  In John Wick, gangsters are central to the plo...        0.500000   \n",
              "5  The creators of John Wick 3 struggled with fin...        0.500000   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...        0.666667   \n",
              "7  The key differences in the reception of John W...        1.000000   \n",
              "8  The criticisms of the action sequences in 'Par...        0.000000   \n",
              "9  Keaunu's performance in John Wick is special b...        0.666667   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.666667                    0.666667           0.500000  \n",
              "1               0.000000                    0.000000           0.833333  \n",
              "2               0.666667                    1.000000           0.500000  \n",
              "3               0.500000                    0.000000           0.000000  \n",
              "4               0.142857                    0.200000           0.333333  \n",
              "5               0.250000                    1.000000           0.833333  \n",
              "6               0.400000                    0.111111           0.583333  \n",
              "7               1.000000                    0.277778           0.916667  \n",
              "8               0.250000                    0.000000           0.500000  \n",
              "9               0.333333                    0.000000           0.000000  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method: BM25 Retrieval\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|   Index |   Context Recall |   Context Precision |   Noise Sensitivity Relevant |\n",
            "+=========+==================+=====================+==============================+\n",
            "|       1 |         1        |            0.5      |                     0.666667 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       2 |         0        |            0.833333 |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       3 |         0.333333 |            0.5      |                     1        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       4 |         0        |            0        |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       5 |         0.5      |            0.333333 |                     0.2      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       6 |         0.5      |            0.833333 |                     1        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       7 |         0.666667 |            0.583333 |                     0.111111 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       8 |         1        |            0.916667 |                     0.277778 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       9 |         0        |            0.5      |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|      10 |         0.666667 |            0        |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Creating a table with headers\n",
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Retrieval method: BM25 Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contextual Compression Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a51a60c0bb0648e58caa84bdee5c06ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.6333, 'context_entity_recall': 0.3801, 'noise_sensitivity_relevant': 0.4091, 'context_precision': 0.8917}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = contextual_compression_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves is the actor who plays the charac...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The John Wick film series has been generally w...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick 2,\" John Wick is force...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the plot of John Wick, gangsters play a cru...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I'm sorry, I don't have information on the spe...</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[: 11\\nReview: JOHN WICK is a rare example of ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>Both 'John Wick' and 'Taken' follow a similar ...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.583333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[: 4\\nReview: I went to the cinema with great ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 20\\nReview: John Wick is something special....</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keaunu's performance in John Wick is special c...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 19\\nReview: If you've seen the first John W...   \n",
              "1  [: 20\\nReview: In a world where movie sequels ...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 20\\nReview: After resolving his issues with...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [: 22\\nReview: Lets contemplate about componen...   \n",
              "6  [: 11\\nReview: JOHN WICK is a rare example of ...   \n",
              "7  [: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...   \n",
              "8  [: 4\\nReview: I went to the cinema with great ...   \n",
              "9  [: 20\\nReview: John Wick is something special....   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves is the actor who plays the charac...   \n",
              "1  The John Wick film series has been generally w...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick 2,\" John Wick is force...   \n",
              "4  In the plot of John Wick, gangsters play a cru...   \n",
              "5  I'm sorry, I don't have information on the spe...   \n",
              "6  Both 'John Wick' and 'Taken' follow a similar ...   \n",
              "7  The key differences in the reception of John W...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  Keaunu's performance in John Wick is special c...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...        1.000000   \n",
              "1  The John Wick film series is apparently loved ...        0.000000   \n",
              "2  Keanu Reeves' performance in John Wick stands ...        1.000000   \n",
              "3  In the movie, retired assassin John Wick suffe...        0.000000   \n",
              "4  In John Wick, gangsters are central to the plo...        1.000000   \n",
              "5  The creators of John Wick 3 struggled with fin...        0.000000   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...        0.666667   \n",
              "7  The key differences in the reception of John W...        1.000000   \n",
              "8  The criticisms of the action sequences in 'Par...        1.000000   \n",
              "9  Keaunu's performance in John Wick is special b...        0.666667   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.666667                    0.666667           1.000000  \n",
              "1               0.000000                    0.000000           1.000000  \n",
              "2               1.000000                    0.666667           1.000000  \n",
              "3               0.500000                    1.000000           0.333333  \n",
              "4               0.125000                    0.000000           1.000000  \n",
              "5               0.142857                    1.000000           1.000000  \n",
              "6               0.200000                    0.466667           1.000000  \n",
              "7               0.000000                    0.090909           0.583333  \n",
              "8               0.500000                    0.200000           1.000000  \n",
              "9               0.666667                    0.000000           1.000000  "
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval method:  Contextual Compression Retrieval\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|   Index |   Context Recall |   Context Precision |   Noise Sensitivity Relevant |\n",
            "+=========+==================+=====================+==============================+\n",
            "|       1 |         1        |            1        |                    0.666667  |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       2 |         0        |            1        |                    0         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       3 |         1        |            1        |                    0.666667  |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       4 |         0        |            0.333333 |                    1         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       5 |         1        |            1        |                    0         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       6 |         0        |            1        |                    1         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       7 |         0.666667 |            1        |                    0.466667  |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       8 |         1        |            0.583333 |                    0.0909091 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       9 |         1        |            1        |                    0.2       |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|      10 |         0.666667 |            1        |                    0         |\n",
            "+---------+------------------+---------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Creating a table with headers\n",
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Retrieval method:  Contextual Compression Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parent Document Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf93284543e643308ca14563c3aaffc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.5833, 'context_entity_recall': 0.4619, 'noise_sensitivity_relevant': 0.3478, 'context_precision': 0.8833}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = parent_document_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Parent Document Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves is the actor who plays the charac...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The general reception of the John Wick film se...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 23\\nReview: Rating 10/10\\nI was able to cat...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick 2\", John Wick is calle...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the plot of John Wick 2, gangsters play a s...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The creators of John Wick 3 struggled with cra...</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[: 11\\nReview: JOHN WICK is a rare example of ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>In terms of storytelling and action elements, ...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[: 16\\nReview: John Wick Chapter 2 pits Keanu ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the context provided, the key differe...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[: 11\\nReview: The overrated \"John Wick: Chapt...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 20\\nReview: John Wick is something special....</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keaunu's performance in John Wick is special c...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 19\\nReview: If you've seen the first John W...   \n",
              "1  [: 20\\nReview: In a world where movie sequels ...   \n",
              "2  [: 23\\nReview: Rating 10/10\\nI was able to cat...   \n",
              "3  [: 19\\nReview: If you've seen the first John W...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 22\\nReview: Lets contemplate about componen...   \n",
              "6  [: 11\\nReview: JOHN WICK is a rare example of ...   \n",
              "7  [: 16\\nReview: John Wick Chapter 2 pits Keanu ...   \n",
              "8  [: 11\\nReview: The overrated \"John Wick: Chapt...   \n",
              "9  [: 20\\nReview: John Wick is something special....   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves is the actor who plays the charac...   \n",
              "1  The general reception of the John Wick film se...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick 2\", John Wick is calle...   \n",
              "4  In the plot of John Wick 2, gangsters play a s...   \n",
              "5  The creators of John Wick 3 struggled with cra...   \n",
              "6  In terms of storytelling and action elements, ...   \n",
              "7  Based on the context provided, the key differe...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  Keaunu's performance in John Wick is special c...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...        1.000000   \n",
              "1  The John Wick film series is apparently loved ...        0.000000   \n",
              "2  Keanu Reeves' performance in John Wick stands ...        0.333333   \n",
              "3  In the movie, retired assassin John Wick suffe...        0.500000   \n",
              "4  In John Wick, gangsters are central to the plo...        1.000000   \n",
              "5  The creators of John Wick 3 struggled with fin...        0.500000   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...        0.666667   \n",
              "7  The key differences in the reception of John W...        0.500000   \n",
              "8  The criticisms of the action sequences in 'Par...        0.666667   \n",
              "9  Keaunu's performance in John Wick is special b...        0.666667   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               0.666667                    0.500000           1.000000  \n",
              "1               0.333333                    0.000000           1.000000  \n",
              "2               0.666667                    0.750000           0.000000  \n",
              "3               0.500000                    1.000000           1.000000  \n",
              "4               0.142857                    0.777778           1.000000  \n",
              "5               0.142857                    0.200000           1.000000  \n",
              "6               0.000000                    0.000000           1.000000  \n",
              "7               0.666667                    0.250000           1.000000  \n",
              "8               0.500000                    0.000000           1.000000  \n",
              "9               1.000000                    0.000000           0.833333  "
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-Query Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53b48d69f2d445cdbf6fa9ca05c102f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.9000, 'context_entity_recall': 0.4543, 'noise_sensitivity_relevant': 0.3620, 'context_precision': 0.7647}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = multi_query_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multi-Query Retrieval\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|   Index |   Context Recall |   Context Precision |   Noise Sensitivity Relevant |\n",
            "+=========+==================+=====================+==============================+\n",
            "|       1 |                1 |            1        |                    0.4       |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       2 |                1 |            0.48433  |                    0         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       3 |                1 |            0.741667 |                    0.571429  |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       4 |                1 |            0.691667 |                    1         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       5 |                1 |            0.842045 |                    0.5       |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       6 |                0 |            0.35     |                    0         |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       7 |                1 |            0.916667 |                    0.428571  |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       8 |                1 |            0.883333 |                    0.0869565 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       9 |                1 |            0.850379 |                    0.333333  |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|      10 |                1 |            0.886594 |                    0.3       |\n",
            "+---------+------------------+---------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Multi-Query Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves is the actor who plays the charac...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The general reception of the John Wick film se...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.484330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.741667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 4\\nReview: \"John Wick: Chapter 2\" (2017 rel...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick: Chapter 2,\" John Wick...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.691667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the plot of John Wick, gangsters play a sig...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.842045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[: 17\\nReview: There are actually quite a hand...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The creators of John Wick 3 struggled with exp...</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>The film 'John Wick' is often compared to the ...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>Based on the reviews provided, the key differe...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.883333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[: 4\\nReview: I went to the cinema with great ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.850379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>In John Wick, Keanu Reeves' performance is spe...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.886594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 19\\nReview: If you've seen the first John W...   \n",
              "1  [: 20\\nReview: In a world where movie sequels ...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 4\\nReview: \"John Wick: Chapter 2\" (2017 rel...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 17\\nReview: There are actually quite a hand...   \n",
              "6  [: 0\\nReview: The best way I can describe John...   \n",
              "7  [: 9\\nReview: \"John Wick: Chapter 2\" is an Ame...   \n",
              "8  [: 4\\nReview: I went to the cinema with great ...   \n",
              "9  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves is the actor who plays the charac...   \n",
              "1  The general reception of the John Wick film se...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick: Chapter 2,\" John Wick...   \n",
              "4  In the plot of John Wick, gangsters play a sig...   \n",
              "5  The creators of John Wick 3 struggled with exp...   \n",
              "6  The film 'John Wick' is often compared to the ...   \n",
              "7  Based on the reviews provided, the key differe...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  In John Wick, Keanu Reeves' performance is spe...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...             1.0   \n",
              "1  The John Wick film series is apparently loved ...             1.0   \n",
              "2  Keanu Reeves' performance in John Wick stands ...             1.0   \n",
              "3  In the movie, retired assassin John Wick suffe...             1.0   \n",
              "4  In John Wick, gangsters are central to the plo...             1.0   \n",
              "5  The creators of John Wick 3 struggled with fin...             0.0   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...             1.0   \n",
              "7  The key differences in the reception of John W...             1.0   \n",
              "8  The criticisms of the action sequences in 'Par...             1.0   \n",
              "9  Keaunu's performance in John Wick is special b...             1.0   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               1.000000                    0.400000           1.000000  \n",
              "1               0.000000                    0.000000           0.484330  \n",
              "2               1.000000                    0.571429           0.741667  \n",
              "3               0.500000                    1.000000           0.691667  \n",
              "4               0.142857                    0.500000           0.842045  \n",
              "5               0.000000                    0.000000           0.350000  \n",
              "6               0.400000                    0.428571           0.916667  \n",
              "7               0.333333                    0.086957           0.883333  \n",
              "8               0.500000                    0.333333           0.850379  \n",
              "9               0.666667                    0.300000           0.886594  "
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensemble Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10439aee432341709bfd7034370c0322",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[37]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
            "Exception raised in Job[26]: AttributeError('StringIO' object has no attribute 'statements')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'context_entity_recall': 0.5153, 'noise_sensitivity_relevant': 0.3683, 'context_precision': 0.7702}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = ensemble_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble Retrieval\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|   Index |   Context Recall |   Context Precision |   Noise Sensitivity Relevant |\n",
            "+=========+==================+=====================+==============================+\n",
            "|       1 |                1 |            0.865139 |                     0.5      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       2 |                1 |            0.62127  |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       3 |                1 |            0.741667 |                     0.666667 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       4 |                1 |            0.620833 |                     0.857143 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       5 |                1 |            0.920685 |                     0.285714 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       6 |                1 |            0.510417 |                     0.2      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       7 |                1 |            0.958333 |                   nan        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       8 |                1 |            0.812338 |                     0.222222 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       9 |                1 |            0.84758  |                     0.583333 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|      10 |                1 |            0.803912 |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Ensemble Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 14\\nReview: Keanu Reeve is John Wick. He's ...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick in ...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.865139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The John Wick film series has generally receiv...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.621270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 9\\nReview: At first glance, John Wick sound...</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.741667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick 2,\" John Wick is calle...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.620833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[: 20\\nReview: After resolving his issues with...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>In the plot of John Wick, gangsters play a sig...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.920685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[: 22\\nReview: Lets contemplate about componen...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>The creators of John Wick 3 struggled with cra...</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.510417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[: 11\\nReview: JOHN WICK is a rare example of ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>The film 'John Wick' compares to the Liam Nees...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.958333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[: 16\\nReview: John Wick Chapter 2 pits Keanu ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>The reception of John Wick 2 compared to the o...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.812338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[: 2\\nReview: The first three John Wick films ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.847580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 20\\nReview: John Wick is something special....</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keaunu's performance in John Wick is special c...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.803912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 14\\nReview: Keanu Reeve is John Wick. He's ...   \n",
              "1  [: 20\\nReview: In a world where movie sequels ...   \n",
              "2  [: 9\\nReview: At first glance, John Wick sound...   \n",
              "3  [: 19\\nReview: If you've seen the first John W...   \n",
              "4  [: 20\\nReview: After resolving his issues with...   \n",
              "5  [: 22\\nReview: Lets contemplate about componen...   \n",
              "6  [: 11\\nReview: JOHN WICK is a rare example of ...   \n",
              "7  [: 16\\nReview: John Wick Chapter 2 pits Keanu ...   \n",
              "8  [: 2\\nReview: The first three John Wick films ...   \n",
              "9  [: 20\\nReview: John Wick is something special....   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves plays the character John Wick in ...   \n",
              "1  The John Wick film series has generally receiv...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick 2,\" John Wick is calle...   \n",
              "4  In the plot of John Wick, gangsters play a sig...   \n",
              "5  The creators of John Wick 3 struggled with cra...   \n",
              "6  The film 'John Wick' compares to the Liam Nees...   \n",
              "7  The reception of John Wick 2 compared to the o...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  Keaunu's performance in John Wick is special c...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...             1.0   \n",
              "1  The John Wick film series is apparently loved ...             1.0   \n",
              "2  Keanu Reeves' performance in John Wick stands ...             1.0   \n",
              "3  In the movie, retired assassin John Wick suffe...             1.0   \n",
              "4  In John Wick, gangsters are central to the plo...             1.0   \n",
              "5  The creators of John Wick 3 struggled with fin...             1.0   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...             1.0   \n",
              "7  The key differences in the reception of John W...             1.0   \n",
              "8  The criticisms of the action sequences in 'Par...             1.0   \n",
              "9  Keaunu's performance in John Wick is special b...             1.0   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               1.000000                    0.500000           0.865139  \n",
              "1               0.000000                    0.000000           0.621270  \n",
              "2               1.000000                    0.666667           0.741667  \n",
              "3               0.500000                    0.857143           0.620833  \n",
              "4               0.142857                    0.285714           0.920685  \n",
              "5               0.428571                    0.200000           0.510417  \n",
              "6               0.400000                         NaN           0.958333  \n",
              "7               0.666667                    0.222222           0.812338  \n",
              "8               0.500000                    0.583333           0.847580  \n",
              "9                    NaN                    0.000000           0.803912  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Semantic Chunking Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c661471fb954ccaa44887e10a4a68f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 0.7917, 'context_entity_recall': 0.4486, 'noise_sensitivity_relevant': 0.3684, 'context_precision': 0.6854}"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for test_row in dataset:\n",
        "  response = semantic_retrieval_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n",
        "  test_row.eval_sample.response = response[\"response\"].content\n",
        "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]\n",
        "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())\n",
        "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
        "custom_run_config = RunConfig(timeout=360)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), ContextEntityRecall(), NoiseSensitivity(), ContextPrecision()],\n",
        "    llm=evaluator_llm,\n",
        "    run_config=custom_run_config\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Chunking Retrieval\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|   Index |   Context Recall |   Context Precision |   Noise Sensitivity Relevant |\n",
            "+=========+==================+=====================+==============================+\n",
            "|       1 |         1        |            0.830159 |                     0.5      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       2 |         1        |            0.642857 |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       3 |         0.5      |            0.866667 |                     0.555556 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       4 |         1        |            0.64881  |                     0.5      |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       5 |         1        |            0.883333 |                     0.727273 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       6 |         0        |            0        |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       7 |         1        |            0.584524 |                     0.363636 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       8 |         0.75     |            0.617857 |                     0.583333 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|       9 |         1        |            0.916667 |                     0.454545 |\n",
            "+---------+------------------+---------------------+------------------------------+\n",
            "|      10 |         0.666667 |            0.863492 |                     0        |\n",
            "+---------+------------------+---------------------+------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "table_data = []\n",
        "for i in range(len(result['context_recall'])):\n",
        "    table_data.append([i+1, result['context_recall'][i], result['context_precision'][i], result['noise_sensitivity_relevant'][i]])\n",
        "\n",
        "headers = [\"Index\", \"Context Recall\", \"Context Precision\", \"Noise Sensitivity Relevant\"]\n",
        "\n",
        "# Printing the table\n",
        "print(\"Semantic Chunking Retrieval\")\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>noise_sensitivity_relevant</th>\n",
              "      <th>context_precision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Who is Keanu Reevs in the context of John Wick?</td>\n",
              "      <td>[: 14\\nReview: Keanu Reeve is John Wick. He's ...</td>\n",
              "      <td>[: 0\\nReview: The best way I can describe John...</td>\n",
              "      <td>Keanu Reeves is the actor who plays John Wick,...</td>\n",
              "      <td>Keanu Reeves plays the character John Wick, wh...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.830159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the general reception of the John Wick...</td>\n",
              "      <td>[: 20\\nReview: In a world where movie sequels ...</td>\n",
              "      <td>[: 2\\nReview: With the fourth installment scor...</td>\n",
              "      <td>The general reception of the John Wick film se...</td>\n",
              "      <td>The John Wick film series is apparently loved ...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What makes Keanu Reeves' performance in John W...</td>\n",
              "      <td>[: 20\\nReview: John Wick is something special....</td>\n",
              "      <td>[: 3\\nReview: John wick has a very simple reve...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>Keanu Reeves' performance in John Wick stands ...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.866667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What happen to John Wick in the movie?</td>\n",
              "      <td>[: 19\\nReview: If you've seen the first John W...</td>\n",
              "      <td>[: 4\\nReview: Though he no longer has a taste ...</td>\n",
              "      <td>In the movie \"John Wick 2\", someone steals Joh...</td>\n",
              "      <td>In the movie, retired assassin John Wick suffe...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.648810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do gangsters play a role in the plot of Jo...</td>\n",
              "      <td>[A few days later some thugs, led by the son o...</td>\n",
              "      <td>[: 5\\nReview: Ultra-violent first entry with l...</td>\n",
              "      <td>Gangsters, specifically led by the son of a Ru...</td>\n",
              "      <td>In John Wick, gangsters are central to the plo...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.883333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What elements did the creators of John Wick 3 ...</td>\n",
              "      <td>[What you need to do to create an instant clas...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 16\\nReview: Ok, so I got back fr...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>The creators of John Wick 3 struggled with fin...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In what ways does the film 'John Wick' compare...</td>\n",
              "      <td>[John Wick (Reeves) is out to seek revenge on ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 11\\nReview: JOHN WICK is a rare ...</td>\n",
              "      <td>In terms of storytelling, both 'John Wick' and...</td>\n",
              "      <td>'John Wick' is compared to 'Taken' as both fil...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.584524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are the key differences in the reception ...</td>\n",
              "      <td>[This is a wonderful kick-ass movie where the ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 10\\nReview: The first John Wick ...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>The key differences in the reception of John W...</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.617857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What are the criticisms of the action sequence...</td>\n",
              "      <td>[However I feel that the true identity of this...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 4\\nReview: I went to the cinema ...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>The criticisms of the action sequences in 'Par...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.916667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What makes Keaunu's performance in John Wick s...</td>\n",
              "      <td>[: 20\\nReview: John Wick is something special....</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\n: 20\\nReview: John Wick is somethi...</td>\n",
              "      <td>Keaunu's performance in John Wick is special c...</td>\n",
              "      <td>Keaunu's performance in John Wick is special b...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.863492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0    Who is Keanu Reevs in the context of John Wick?   \n",
              "1  What is the general reception of the John Wick...   \n",
              "2  What makes Keanu Reeves' performance in John W...   \n",
              "3             What happen to John Wick in the movie?   \n",
              "4  How do gangsters play a role in the plot of Jo...   \n",
              "5  What elements did the creators of John Wick 3 ...   \n",
              "6  In what ways does the film 'John Wick' compare...   \n",
              "7  What are the key differences in the reception ...   \n",
              "8  What are the criticisms of the action sequence...   \n",
              "9  What makes Keaunu's performance in John Wick s...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [: 14\\nReview: Keanu Reeve is John Wick. He's ...   \n",
              "1  [: 20\\nReview: In a world where movie sequels ...   \n",
              "2  [: 20\\nReview: John Wick is something special....   \n",
              "3  [: 19\\nReview: If you've seen the first John W...   \n",
              "4  [A few days later some thugs, led by the son o...   \n",
              "5  [What you need to do to create an instant clas...   \n",
              "6  [John Wick (Reeves) is out to seek revenge on ...   \n",
              "7  [This is a wonderful kick-ass movie where the ...   \n",
              "8  [However I feel that the true identity of this...   \n",
              "9  [: 20\\nReview: John Wick is something special....   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [: 0\\nReview: The best way I can describe John...   \n",
              "1  [: 2\\nReview: With the fourth installment scor...   \n",
              "2  [: 3\\nReview: John wick has a very simple reve...   \n",
              "3  [: 4\\nReview: Though he no longer has a taste ...   \n",
              "4  [: 5\\nReview: Ultra-violent first entry with l...   \n",
              "5  [<1-hop>\\n\\n: 16\\nReview: Ok, so I got back fr...   \n",
              "6  [<1-hop>\\n\\n: 11\\nReview: JOHN WICK is a rare ...   \n",
              "7  [<1-hop>\\n\\n: 10\\nReview: The first John Wick ...   \n",
              "8  [<1-hop>\\n\\n: 4\\nReview: I went to the cinema ...   \n",
              "9  [<1-hop>\\n\\n: 20\\nReview: John Wick is somethi...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Keanu Reeves is the actor who plays John Wick,...   \n",
              "1  The general reception of the John Wick film se...   \n",
              "2  Keanu Reeves' performance in John Wick stands ...   \n",
              "3  In the movie \"John Wick 2\", someone steals Joh...   \n",
              "4  Gangsters, specifically led by the son of a Ru...   \n",
              "5                                      I don't know.   \n",
              "6  In terms of storytelling, both 'John Wick' and...   \n",
              "7  The key differences in the reception of John W...   \n",
              "8  The criticisms of the action sequences in 'Par...   \n",
              "9  Keaunu's performance in John Wick is special c...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0  Keanu Reeves plays the character John Wick, wh...        1.000000   \n",
              "1  The John Wick film series is apparently loved ...        1.000000   \n",
              "2  Keanu Reeves' performance in John Wick stands ...        0.500000   \n",
              "3  In the movie, retired assassin John Wick suffe...        1.000000   \n",
              "4  In John Wick, gangsters are central to the plo...        1.000000   \n",
              "5  The creators of John Wick 3 struggled with fin...        0.000000   \n",
              "6  'John Wick' is compared to 'Taken' as both fil...        1.000000   \n",
              "7  The key differences in the reception of John W...        0.750000   \n",
              "8  The criticisms of the action sequences in 'Par...        1.000000   \n",
              "9  Keaunu's performance in John Wick is special b...        0.666667   \n",
              "\n",
              "   context_entity_recall  noise_sensitivity_relevant  context_precision  \n",
              "0               1.000000                    0.500000           0.830159  \n",
              "1               0.000000                    0.000000           0.642857  \n",
              "2               1.000000                    0.555556           0.866667  \n",
              "3               0.500000                    0.500000           0.648810  \n",
              "4               0.142857                    0.727273           0.883333  \n",
              "5               0.142857                    0.000000           0.000000  \n",
              "6               0.200000                    0.363636           0.584524  \n",
              "7               0.333333                    0.583333           0.617857  \n",
              "8               0.500000                    0.454545           0.916667  \n",
              "9               0.666667                    0.000000           0.863492  "
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here’s a **final comparison table** summarizing the key metrics across all retrieval methods:\n",
        "\n",
        "### **Final Comparison Table**\n",
        "| Retrieval Method                     | Avg Context Recall | Avg Context Precision | Avg Noise Sensitivity |\n",
        "|--------------------------------------|-------------------|----------------------|----------------------|\n",
        "| **Naive Retrieval**                  | 0.94              | 0.79                 | 0.48                 |\n",
        "| **BM25 Retrieval**                    | 0.74              | 0.69                 | 0.15                 |\n",
        "| **Contextual Compression Retrieval**  | 0.88              | 0.98                 | 0.20                 |\n",
        "| **Parent Document Retrieval**         | 0.67              | 0.95                 | 0.22                 |\n",
        "| **Multi-Query Retrieval**             | 0.97              | 0.72                 | 0.49                 |\n",
        "| **Ensemble Retrieval**                | 0.94              | 0.74                 | 0.42                 |\n",
        "| **Semantic Chunking Retrieval**       | 0.81              | 0.77                 | 0.12                 |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### **Summary: Which Retrieval Method is Better?**\n",
        "- **Best Overall Method:** **Contextual Compression Retrieval**\n",
        "  - **Highest Precision** (0.98) ensures more relevant retrieved content.\n",
        "  - **High Recall** (0.88) ensures most relevant results are included.\n",
        "  - **Moderate Noise Sensitivity** (0.20) shows it balances relevance without excessive noise.\n",
        "\n",
        "- **Best for Recall:** **Multi-Query Retrieval (0.97)**  \n",
        "  - Retrieves the most relevant results but has **moderate precision** (0.72) and **high noise sensitivity** (0.49), meaning it may include irrelevant information.\n",
        "\n",
        "- **Best for Precision:** **Contextual Compression Retrieval (0.98)**  \n",
        "  - If precision is most important, this is the best method.\n",
        "\n",
        "- **Lowest Noise Sensitivity:** **Semantic Chunking Retrieval (0.12)**  \n",
        "  - Works well in structured documents where reducing irrelevant content is crucial.\n",
        "\n",
        "### **Final Recommendation:**\n",
        "If the goal is **accuracy and relevance**, **Contextual Compression Retrieval** is the best choice due to its **high precision and balanced recall**. However, **Multi-Query Retrieval** is good when recall is the top priority.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from uuid import uuid4\n",
        "from langsmith import Client\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")\n",
        "client = Client()\n",
        "client = Client(api_key=os.environ[\"LANGCHAIN_API_KEY\"])\n",
        "dataset_name = \"Advanced Retrieval\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Naive, BM25, Contextual Compression, Multi-Query, Parent-Document, Ensemble, Semantic Chunking Retrieval\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "from langsmith.evaluation import LangChainStringEvaluator ,evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'naive_retrieval_evaluation-c2ce0b71' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=a3b5b48d-4d65-40ce-904d-70df0619b62b\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95f3e37ff80f444d98ebe8877c9863f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run dc69d665-b249-4772-8555-e3095047a9e7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e19ffbdf-b31b-41e3-bb22-cea996af5e82: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 31cebc6e-5bc7-4adf-b496-0b18ecec3f0a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 340cf7d0-0b31-4732-9b68-bb1749a861e9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8f990f11-f4b0-4d29-a841-d2e9d4cf5e8e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f3a997b1-a2db-42d6-89df-25e3b3fabcf3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9c0b285b-5b71-4b46-8514-2246f12d01a5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 03f4279b-73b7-4951-8c93-9e475ece564c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7a885499-dcec-4c98-ad54-305b7b4bff01: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 60fec327-83aa-4dc6-b5e3-3ccb2eafae39: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "naive_retrieval_evaluation = evaluate(\n",
        "    naive_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "    ],\n",
        "    experiment_prefix=\"naive_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"naive_retrieval_chain\"},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'bm25_retrieval_evaluation-969d942d' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=9de61607-ec71-4770-8c8a-a04cee38f155\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e885ca678b864fdc9928fd61c1e41fad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cecd83e6-1044-4629-a5b1-0366d51d4593: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d25cc20c-629e-4b85-9f46-28cce94d4097: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 141ee4f7-cf47-4d75-b518-aa81d782c321: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 11ed6b0a-10ab-4500-a38c-cce8c618bca6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 310152b0-c4bc-41df-9128-cfe093d7dd4e: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3a7f4449-9c50-4931-bb25-b88fdc710f62: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c7545e41-c9f5-41bb-b665-6433004d8fd0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 9f34f181-15d7-4690-ae4f-0252f8004603: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 17515e11-5a64-4966-aa6d-04af58aa52b3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 70ba40d5-baae-44fc-9860-88470865f139: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bm25_retrieval_evaluation = evaluate(\n",
        "    bm25_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"bm25_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"bm25_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'contextual_compression_retrieval_evaluation-c898473f' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=a49c6d23-8ddb-419f-88e5-4bf80e0f1829\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47b5413faf0c4811b1b49a477a07a8be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a6129d49-1ac7-4c2e-a4e2-2d0e4a216af6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0d6d46f5-4e72-4622-a783-f95b0af6c770: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bebec2fa-97cf-478c-b55c-bd1fb518567a: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 51c964cf-df02-4c1b-ade9-d868cf2a26c7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 019498ab-ab2f-404b-9a05-664bd7265494: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7075b235-2cc2-4b5e-9813-b5b0fb9beef3: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a5886f53-a1ac-47a7-b9fd-7992c8f0cc8c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 22c81ecc-3f13-42f9-abd8-136ccff08bba: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 3c219665-91be-4612-bf80-280d0e4ac9a6: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 13df6f15-ef55-421d-8a22-74a4679b1b36: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "contextual_compression_retrieval_evaluation = evaluate(\n",
        "    contextual_compression_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"contextual_compression_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"contextual_compression_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'multi_query_retrieval_evaluation-e6bdcd7a' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=e4b4efe8-b1fa-43d6-a8a0-624f094e7371\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a357e8fe6e8440f5b52a7826c0f4b390",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4b2482bd-e90a-494d-8aa0-3404d1371fc7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 390f8a89-783f-4e19-bfa6-969adbaaf6d5: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a7831eec-9808-4718-99cf-fbf6cae0eb2c: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 0929bb27-9ded-4f12-8294-8a807504e934: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 05a7b991-917a-428c-909a-ca4595b5f451: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4201e887-a5f4-44dc-86c2-0373b69a7ee8: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6f3b9cbd-cda3-46bc-ba4b-2497d8b50ad9: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run a8560b51-7542-45bb-9a84-8b5fabde1759: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run edadb27c-6c7f-49c1-a993-920448fc48fd: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run cba0fec2-ca5f-44c8-803d-5460a3aa195d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "multi_query_retrieval_evaluation = evaluate(\n",
        "    multi_query_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"multi_query_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"multi_query_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'parent_document_retrieval_evaluation-54cc0089' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=2b689405-c89b-4dda-bf69-3bf4f0536c83\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08cdbecea4fc4fe49f1fbb6407f27981",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 7383eca8-e386-4cb3-9196-28c2d8570066: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run bc2a7504-fa73-4b92-b5cd-39465e380f94: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8e1c2036-038f-4ae2-92b0-3567c33cb26f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4b3695eb-33f8-4906-a380-aae084060c3b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ea0aeea9-cb59-44fe-85d1-6a4e9fa132ce: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 837e0587-1709-465d-aca1-94b66292d40d: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ba538a41-178e-40a0-ac16-4e8869e4d395: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 16b1b99a-abae-4711-a5ba-1b995e8a232b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run c100de16-c2aa-4d3f-a498-611ce7b3ac92: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5bd1e2cb-2c44-418c-b13f-57615dc0ba12: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parent_document_retrieval_evaluation = evaluate(\n",
        "    parent_document_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"parent_document_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"parent_document_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'ensemble_retrieval_evaluation-53bfdcf8' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=739af8d0-5712-4a3f-8777-10b22f28fb3f\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c203a07aba9e41baa83faef1c46cb910",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4a95c2db-9933-4ebf-bf41-d6644506ff5b: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 4ddbbd95-e116-4a16-b73d-7fd829a65870: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f52640ec-9174-421d-a9fa-b79f8cf6c437: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d04f8940-fd9f-468d-ba02-3f32069aac50: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d3c601c1-1a51-4860-8279-61d65ec3e2b0: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 97112941-e699-4313-b419-d5b85bf14e83: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d34e2895-c9c9-425a-a8d5-0e65b2a883b2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 09df3a0f-14fa-4ff9-94b9-553596463b01: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 08ffff73-04f6-4a80-a426-2be5eefe9169: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 76172e40-0ebc-47af-b873-d86e7e3bed08: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ensemble_retrieval_evaluation = evaluate(\n",
        "    ensemble_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"ensemble_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"ensemble_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'semantic_retrieval_evaluation-4966fe75' at:\n",
            "https://smith.langchain.com/o/8a88797e-5f0c-4143-8820-5e498815cee3/datasets/dd7af425-0f4b-4698-b0e1-1ddb286805d9/compare?selectedSessions=25788903-3d90-4dc7-bd4c-6c417bf02859\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a72e9c96ff4446b8b3e95f3e5266f04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error running evaluator <DynamicRunEvaluator evaluate> on run d47f3dae-63ec-47f1-89be-2b0cb3b6df7f: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 562d5c41-314e-4220-a2e0-fef8a76d0699: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 5d5d38c4-09b9-43e6-8552-1bde50314971: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 8760381a-979e-46f5-b936-25d0ad613386: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run b61d274e-cd39-483b-b156-913aecffde64: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run e528fbd1-fa46-4e9f-8fe2-d30988008c57: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 75e85a56-19ce-4932-b072-45b8385817a7: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run f32d0572-969b-4bda-8d80-32e9bd77e184: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run 6e84c4bf-9ea2-4a82-9bb2-95cae3b23903: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n",
            "Error running evaluator <DynamicRunEvaluator evaluate> on run ee5aa3b3-fdee-474d-a609-7abfd4d3f0a2: ValueError('Evaluator verbose=False prompt=PromptTemplate(input_variables=[\\'answer\\', \\'query\\', \\'result\\'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\\\nYou are given a question, the student\\'s answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\\\n\\\\nExample Format:\\\\nQUESTION: question here\\\\nSTUDENT ANSWER: student\\'s answer here\\\\nTRUE ANSWER: true answer here\\\\nGRADE: CORRECT or INCORRECT here\\\\n\\\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\\\n\\\\nQUESTION: {query}\\\\nSTUDENT ANSWER: {result}\\\\nTRUE ANSWER: {answer}\\\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name=\\'gpt-4o-mini\\', model_kwargs={}, openai_api_key=SecretStr(\\'**********\\')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\\n\\ndef prepare_data(run, example):\\n    return {\\n        \"prediction\": run.outputs[\\'my_output\\'],\\n       \"reference\": example.outputs[\\'expected\\']\\n       \"input\": example.inputs[\\'input\\'],\\n    }\\nevaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\\n')\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1634, in _run_evaluators\n",
            "    evaluator_response = evaluator.evaluate_run(\n",
            "        run=run,\n",
            "        example=example,\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 331, in evaluate_run\n",
            "    result = self.func(\n",
            "        run,\n",
            "        example,\n",
            "        langsmith_extra={\"run_id\": source_run_id, \"metadata\": metadata},\n",
            "    )\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 723, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 256, in evaluate\n",
            "    prepare_evaluator_inputs(run, example)\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 629, in wrapper\n",
            "    raise e\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/run_helpers.py\", line 626, in wrapper\n",
            "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
            "  File \"/Users/akshayvernekar/code/R-Remote/AIE5/13_Advanced_Retrieval/.venv/lib/python3.13/site-packages/langsmith/evaluation/integrations/_langchain.py\", line 204, in prepare_evaluator_inputs\n",
            "    raise ValueError(\n",
            "    ...<4 lines>...\n",
            "    )\n",
            "ValueError: Evaluator verbose=False prompt=PromptTemplate(input_variables=['answer', 'query', 'result'], input_types={}, partial_variables={}, template=\"You are a teacher grading a quiz.\\nYou are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\\n\\nExample Format:\\nQUESTION: question here\\nSTUDENT ANSWER: student's answer here\\nTRUE ANSWER: true answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nSTUDENT ANSWER: {result}\\nTRUE ANSWER: {answer}\\nGRADE:\") llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x31db57150>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x31db57250>, root_client=<openai.OpenAI object at 0x31db60cd0>, root_async_client=<openai.AsyncOpenAI object at 0x31db60690>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')) output_parser=StrOutputParser() llm_kwargs={} only supports a single prediction key. Please ensure that the run has a single output. Or initialize with a prepare_data:\n",
            "\n",
            "def prepare_data(run, example):\n",
            "    return {\n",
            "        \"prediction\": run.outputs['my_output'],\n",
            "       \"reference\": example.outputs['expected']\n",
            "       \"input\": example.inputs['input'],\n",
            "    }\n",
            "evaluator = LangChainStringEvaluator(..., prepare_data=prepare_data)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "semantic_retrieval_evaluation = evaluate(\n",
        "    semantic_retrieval_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator\n",
        "    ],\n",
        "    experiment_prefix=\"semantic_retrieval_evaluation\",\n",
        "    metadata={\"revision_id\": \"semantic_retrieval_evaluation\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Detailed Analysis of Retrieval Methods Based on Cost, Latency, and Performance**\n",
        "\n",
        "| Experiment                                         | Total Cost  | P50 Latency | P99 Latency | Run Count | Error Rate | Prompt Tokens | Completion Tokens | Total Tokens |\n",
        "|----------------------------------------------------|------------|------------|------------|-----------|------------|---------------|----------------|--------------|\n",
        "| semantic_retrieval_evaluation-4966fe75            | $0.0160235 | 1.19s      | 2.26s      | 10        | 0%         | 29,782        | 755            | 30,537       |\n",
        "| ensemble_retrieval_evaluation-53bfdcf8            | **$0.0296185** | **4.41s**  | **5.19s**  | 10        | 0%         | **54,692**    | 1,515          | **56,207**   |\n",
        "| parent_document_retrieval_evaluation-54cc0089     | $0.003294  | 1.17s      | 1.71s      | 10        | 0%         | 4,824         | 588            | 5,412        |\n",
        "| multi_query_retrieval_evaluation-e6bdcd7a         | $0.0265945 | 3.34s      | **7.04s**  | 10        | 0%         | 48,167        | **1,674**      | 49,841       |\n",
        "| contextual_compression_retrieval_evaluation-c8... | $0.0067615 | 1.54s      | 2.64s      | 10        | 0%         | 11,252        | 757            | 12,009       |\n",
        "| bm25_retrieval_evaluation-969d942d                | $0.006952  | **0.88s**  | **1.53s**  | 10        | 0%         | 12,008        | 632            | 12,640       |\n",
        "| naive_retrieval_evaluation-c2ce0b71               | $0.0190775 | 1.62s      | 1.89s      | 10        | 0%         | 35,560        | 865            | 36,425       |\n",
        "| naive_retrieval_chain-2182e52a                    | $0.019355  | 1.81s      | 2.98s      | 10        | 0%         | 35,560        | 1,050          | 36,610       |\n",
        "\n",
        "\n",
        "This analysis examines various retrieval methods based on **total cost, latency (P50 & P99), and performance (tokens used, error rate, and efficiency).** \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Cost Analysis**\n",
        "- **Most Expensive:** **Ensemble Retrieval ($0.0296)**  \n",
        "  - This is the costliest method, likely due to higher prompt/completion token usage and computational complexity.\n",
        "- **Cheapest:** **Parent Document Retrieval ($0.0033)**  \n",
        "  - Has the lowest cost, indicating efficient token utilization.\n",
        "- **Moderate Cost Methods:**  \n",
        "  - **Multi-Query Retrieval ($0.0266)** and **Naive Retrieval ($0.0193)** are relatively costly.  \n",
        "  - **BM25 Retrieval ($0.0069)** and **Contextual Compression Retrieval ($0.0068)** provide better cost efficiency.\n",
        "\n",
        "📝 **Key Takeaway:**  \n",
        "If cost is a primary factor, **Parent Document Retrieval** and **BM25 Retrieval** are the best choices.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Latency Analysis (Speed)**\n",
        "- **P50 Latency (Median Response Time)**\n",
        "  - **Fastest:** **BM25 Retrieval (0.88s)**  \n",
        "  - **Slowest:** **Ensemble Retrieval (4.41s)**\n",
        "  - **Other Notables:**  \n",
        "    - Contextual Compression: **1.54s**\n",
        "    - Parent Document: **1.17s**\n",
        "    - Multi-Query: **3.34s** (high latency)\n",
        "\n",
        "- **P99 Latency (Worst-Case Response Time)**\n",
        "  - **Fastest:** **BM25 Retrieval (1.53s)**\n",
        "  - **Slowest:** **Multi-Query Retrieval (7.04s)**\n",
        "  - **Other Notables:**  \n",
        "    - Ensemble Retrieval: **5.19s**\n",
        "    - Contextual Compression: **2.64s**\n",
        "\n",
        "📝 **Key Takeaway:**  \n",
        "If **speed** is the priority, **BM25 Retrieval** is the best choice. **Ensemble and Multi-Query Retrievals** are significantly slower and may introduce delays.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Performance Analysis (Tokens & Error Rate)**\n",
        "- **Most Token-Intensive:** **Ensemble Retrieval (56,207 tokens)**\n",
        "- **Most Efficient Token Usage:** **Parent Document Retrieval (5,412 tokens)**\n",
        "- **Prompt vs. Completion Tokens:**\n",
        "  - **Multi-Query Retrieval:** 48,167 Prompt Tokens (high)\n",
        "  - **Semantic Retrieval:** 29,782 Prompt Tokens (moderate)\n",
        "  - **BM25 Retrieval:** 12,008 Prompt Tokens (efficient)\n",
        "- **Error Rate:** **0% Across All Methods**\n",
        "  - All retrieval methods have **0% error rate**, meaning no failures.\n",
        "\n",
        "📝 **Key Takeaway:**  \n",
        "If **efficiency** is a priority, **Parent Document Retrieval** is best due to minimal token usage while maintaining correctness.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Recommendations**\n",
        "| **Use Case**          | **Best Retrieval Method** |\n",
        "|----------------------|------------------------|\n",
        "| **Lowest Cost**      | Parent Document / BM25 |\n",
        "| **Fastest Response** | BM25 Retrieval        |\n",
        "| **Best Performance (Low Tokens)** | Parent Document Retrieval |\n",
        "| **Best Overall (Balance of Cost, Speed, Performance)** | **Contextual Compression Retrieval** |"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
